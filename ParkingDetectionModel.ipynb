{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ParkingDetectionModel.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fFHQ5dZVGBik",
        "G3stIPzoHqmT",
        "QhHCIsauGq1E",
        "QvmI5lrcGq1F",
        "I5EkPBFGGq1G",
        "ENV0YrjhGq1G",
        "2hxifguLGq1G",
        "gDva8k_0Gq1H"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "oldHeight": 245.71780199999998,
      "position": {
        "height": "244.712px",
        "left": "727.067px",
        "right": "20px",
        "top": "142.965px",
        "width": "596.006px"
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "varInspector_section_display": "block",
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Girgis98/ParkingSlotDetectionModel/blob/main/ParkingDetectionModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EK07UF2BNUjb",
        "outputId": "9c628d9f-7b21-4495-b768-0772b237b4e7"
      },
      "source": [
        "%config IPCompleter.greedy=True\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\";\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwVPUWVLBojp",
        "outputId": "e5e88594-4dce-4bea-b55e-b0719cdb1bb7"
      },
      "source": [
        "!pip install torch-optimizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch-optimizer\n",
            "  Downloading torch_optimizer-0.1.0-py3-none-any.whl (72 kB)\n",
            "\u001b[?25l\r\u001b[K     |████▌                           | 10 kB 32.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 20 kB 29.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 30 kB 18.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 40 kB 16.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 51 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 61 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 71 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 72 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from torch-optimizer) (1.9.0+cu102)\n",
            "Collecting pytorch-ranger>=0.1.1\n",
            "  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->torch-optimizer) (3.7.4.3)\n",
            "Installing collected packages: pytorch-ranger, torch-optimizer\n",
            "Successfully installed pytorch-ranger-0.1.1 torch-optimizer-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQhqS7FWNUjp"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7-NpoDCJSul"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import torch \n",
        "import torch_optimizer as optim\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import scipy.io\n",
        "import glob\n",
        "from scipy.io import loadmat\n",
        "import scipy\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import cv2\n",
        "import json\n",
        "import os\n",
        "import os.path\n",
        "import cv2 as cv\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, utils\n",
        "import math\n",
        "from skimage import io, transform\n",
        "from collections import namedtuple\n",
        "from enum import Enum\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "from matplotlib import cm\n",
        "import shutil\n",
        "import codecs \n",
        "import timeit\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import progressbar\n",
        "import time\n",
        "import pylab as pl\n",
        "from IPython import display\n",
        "import gc\n",
        "import pickle\n",
        "import random\n",
        "import smtplib, ssl\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLUOpov2NKnm"
      },
      "source": [
        "### Sending Notification Mail"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwKPXUJINiNn"
      },
      "source": [
        "def send_notification():\n",
        "  sender_email = \"girgis.mamdouh@gmail.com\"\n",
        "  receiver_email = \"girgis.mamdouh@gmail.com\"\n",
        "  SUBJECT = \"Code finished\"\n",
        "\n",
        "  message = \"Go continue running.\"\n",
        "  message = 'Subject: {}\\n\\n{}'.format(SUBJECT, message)\n",
        "  # Send email here\n",
        "\n",
        "  port = 465  # For SSL\n",
        "  password = \"jaetmhgdilhannsw\"\n",
        "\n",
        "  # Create a secure SSL context\n",
        "  context = ssl.create_default_context()\n",
        "\n",
        "  with smtplib.SMTP_SSL(\"smtp.gmail.com\", port, context=context) as server:\n",
        "      server.login(sender_email, password)\n",
        "      server.sendmail(sender_email, receiver_email, message)\n",
        "def send_error():\n",
        "  sender_email = \"girgis.mamdouh@gmail.com\"\n",
        "  receiver_email = \"girgis.mamdouh@gmail.com\"\n",
        "  SUBJECT = \"Error\"\n",
        "\n",
        "  message = \"Go rerun.\"\n",
        "  message = 'Subject: {}\\n\\n{}'.format(SUBJECT, message)\n",
        "  # Send email here\n",
        "\n",
        "  port = 465  # For SSL\n",
        "  password = \"jaetmhgdilhannsw\"\n",
        "\n",
        "  # Create a secure SSL context\n",
        "  context = ssl.create_default_context()\n",
        "\n",
        "  with smtplib.SMTP_SSL(\"smtp.gmail.com\", port, context=context) as server:\n",
        "      server.login(sender_email, password)\n",
        "      server.sendmail(sender_email, receiver_email, message)      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GM1IBaUyAJm",
        "outputId": "6b9e7d42-1e16-4bae-a65b-f3d6758b2a49"
      },
      "source": [
        "device = torch.device(\"cuda\") \n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jul 26 06:01:52 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    31W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Wc_frul1Cv1"
      },
      "source": [
        "## ConvNet class\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdYhflSGKAMS"
      },
      "source": [
        "# Convolutional neural network (two convolutional layers)\n",
        "class ConvNet(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        dropout_prob = 0.0\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.1),nn.Dropout(p=dropout_prob))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.1),nn.Dropout(p=dropout_prob))\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 32, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.1),nn.Dropout(p=dropout_prob))\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.1),nn.Dropout(p=dropout_prob))\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.1),nn.Dropout(p=dropout_prob))\n",
        "        self.layer6 = nn.Sequential(\n",
        "            nn.Conv2d(128, 64, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.1),nn.Dropout(p=dropout_prob))\n",
        "        self.layer7 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.1),nn.Dropout(p=dropout_prob))\n",
        "        self.layer8 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.1),nn.Dropout(p=dropout_prob))\n",
        "        self.layer9 = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.1),nn.Dropout(p=dropout_prob))\n",
        "        self.layer10 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.1),nn.Dropout(p=dropout_prob))\n",
        "        self.layer91 = nn.Sequential(\n",
        "            nn.Conv2d(256, 128, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.1),nn.Dropout(p=dropout_prob))\n",
        "        self.layer101 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.1),nn.Dropout(p=dropout_prob))\n",
        "        self.layer11 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.1),nn.Dropout(p=dropout_prob))\n",
        "        self.layer12 = nn.Sequential(\n",
        "            nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.1),nn.Dropout(p=dropout_prob))\n",
        "        self.layer13 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.1),nn.Dropout(p=dropout_prob))\n",
        "        self.layer121 = nn.Sequential(\n",
        "            nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.1),nn.Dropout(p=dropout_prob))\n",
        "        self.layer131 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.1),nn.Dropout(p=dropout_prob))\n",
        "        self.layer14 = nn.Sequential(\n",
        "            nn.Conv2d(512, 1024, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.1),nn.Dropout(p=dropout_prob))\n",
        "        self.layer15 = nn.Sequential(\n",
        "            nn.Conv2d(1024, 512, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.1),nn.Dropout(p=dropout_prob))\n",
        "        self.layer16 = nn.Sequential(\n",
        "            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.1),nn.Dropout(p=dropout_prob))\n",
        "        self.layer151 = nn.Sequential(\n",
        "            nn.Conv2d(1024, 512, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.1),nn.Dropout(p=dropout_prob))\n",
        "        self.layer161 = nn.Sequential(\n",
        "            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.1),nn.Dropout(p=dropout_prob))\n",
        "        self.layer152 = nn.Sequential(\n",
        "            nn.Conv2d(1024, 512, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.1),nn.Dropout(p=dropout_prob))\n",
        "        self.layer162 = nn.Sequential(\n",
        "            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.LeakyReLU(0.1),nn.Dropout(p=dropout_prob))\n",
        "        self.layer17 = nn.Sequential(\n",
        "            nn.Conv2d(1024, 6, kernel_size=1, stride=1, padding=0),\n",
        "            nn.Identity())\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.layer5(out)\n",
        "        out = self.layer6(out)\n",
        "        out = self.layer7(out)\n",
        "        out = self.layer8(out)\n",
        "        out = self.layer9(out)\n",
        "        out = self.layer10(out)\n",
        "        out = self.layer91(out)\n",
        "        out = self.layer101(out)\n",
        "        out = self.layer11(out)\n",
        "        out = self.layer12(out)\n",
        "        out = self.layer13(out)\n",
        "        out = self.layer121(out)\n",
        "        out = self.layer131(out)\n",
        "        out = self.layer14(out)\n",
        "        out = self.layer15(out)\n",
        "        out = self.layer16(out)\n",
        "        out = self.layer151(out)\n",
        "        out = self.layer161(out)\n",
        "        #print(out.shape)\n",
        "        out = self.layer152(out)\n",
        "        #print(out.shape)\n",
        "        out = self.layer162(out)\n",
        "        #print(out.shape)\n",
        "        out = self.layer17(out)\n",
        "        #print(out.shape)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        #print(out.shape)\n",
        "       \n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DZcQc4I1ikD"
      },
      "source": [
        "## Dataset class\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0xvXGnHWoC1"
      },
      "source": [
        "MarkingPoint = namedtuple('MarkingPoint', ['x', 'y', 'direction_x', 'direction_y', 'shape'])\n",
        "class ParkingSlotDataset(Dataset):\n",
        "    \"\"\"Parking slot dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, root, names_ls = []):\n",
        "\n",
        "        super(ParkingSlotDataset, self).__init__()\n",
        "        self.root = root\n",
        "        self.sample_names = names_ls.copy()\n",
        "        self.test_names = []\n",
        "        self.all_samples = []\n",
        "        self.image_transform = ToTensor()\n",
        "        self.mode = 'train'\n",
        "        if len(names_ls) == 0:\n",
        "            accum = 0\n",
        "            sub_dirs_num = (len([f for f in os.listdir(root)]))\n",
        "            print(\"Number of Subdirectories is: \", sub_dirs_num, \"\\n\")\n",
        "            for i in range(sub_dirs_num):\n",
        "                sub_root = f'{root}/{i}'\n",
        "                print('\\nfile is:', i)\n",
        "                for file in os.listdir(sub_root):\n",
        "                    if file.endswith(\".json\"):\n",
        "                        self.sample_names.append(f'{i}/{os.path.splitext(file)[0]}')\n",
        "                print('file len:', len(self.sample_names) - accum)\n",
        "                accum = len(self.sample_names)\n",
        "                print('total len:', len(self.sample_names))\n",
        "\n",
        "    def split(self):\n",
        "        total_num = len(self.sample_names)\n",
        "        train_num = int(0.85 * total_num)\n",
        "\n",
        "        random.Random(7).shuffle(self.sample_names)\n",
        "\n",
        "        self.all_samples = self.sample_names.copy()\n",
        "        self.sample_names = self.all_samples[0: train_num]\n",
        "        self.test_names = self.all_samples[train_num:]\n",
        "\n",
        "    def change_mode(self,mode):\n",
        "      if mode == 'train' and self.mode == 'test':\n",
        "        self.sample_names , self.test_names = self.test_names.copy() , self.sample_names.copy()  \n",
        "        self.mode = 'train'\n",
        "        print(\"Training...\",\"\\nTraining Dataset length is:\",len(self.sample_names))\n",
        "\n",
        "      elif mode == 'test' and self.mode == 'train':  \n",
        "        self.sample_names , self.test_names = self.test_names.copy() , self.sample_names.copy()  \n",
        "        self.mode = 'test'\n",
        "        print(\"Testing...\",\"\\nTesting Dataset length is:\",len(self.sample_names))\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        try:\n",
        "            name = self.sample_names[index]\n",
        "            image = cv.imread(f\"{self.root}/{name}.jpg\")\n",
        "        except:\n",
        "            print('error index is:', index)\n",
        "        try:\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            image2 = cv2.medianBlur(image, 9)\n",
        "            smoothed = cv2.GaussianBlur(image2, (9, 9), 10)\n",
        "            sharp = cv2.addWeighted(image2, 2, smoothed, -1, 0)\n",
        "            bilat = cv2.bilateralFilter(sharp, 9, 75, 75)\n",
        "            image = bilat\n",
        "\n",
        "        except:\n",
        "            print(\"error here\", self.sample_names[index])\n",
        "        image = self.image_transform(image)\n",
        "        marking_points = []\n",
        "        slots = []\n",
        "\n",
        "        with open((f\"{self.root}/{name}.json\"), ) as file:\n",
        "            if not isinstance(json.load(file)['marks'][0], list):\n",
        "                with open((f\"{self.root}/{name}.json\"), ) as file:\n",
        "                    marking_points.append(MarkingPoint(*json.load(file)['marks']))\n",
        "            else:\n",
        "                with open((f\"{self.root}/{name}.json\"), ) as file:\n",
        "                    for label in np.array(json.load(file)['marks']):\n",
        "                        marking_points.append(MarkingPoint(*label))\n",
        "\n",
        "        with open((f\"{self.root}/{name}.json\"), ) as file:\n",
        "            if (len(json.load(file)['slots']) == 0):\n",
        "                pass\n",
        "            else:\n",
        "                with open((f\"{self.root}/{name}.json\"), ) as file:\n",
        "                    if not isinstance(json.load(file)['slots'][0], list):\n",
        "                        with open((f\"{self.root}/{name}.json\"), ) as file:\n",
        "                            slots.append(Slot(*(json.load(file)['slots'])))\n",
        "                    else:\n",
        "                        with open((f\"{self.root}/{name}.json\"), ) as file:\n",
        "                            for label in json.load(file)['slots']:\n",
        "                                slots.append(Slot(*label))\n",
        "        return {'image': image, 'marks': marking_points, 'slots': slots}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sample_names)\n",
        "\n",
        "    def __set__(self, index, value):\n",
        "        # save file name then delete file thrn create another file with new data\n",
        "        name = self.sample_names[index]\n",
        "        in_image = value['image']\n",
        "        in_image = in_image.permute(1, 2, 0)\n",
        "        in_image = cv2.cvtColor(np.array(in_image), cv2.COLOR_RGB2BGR)\n",
        "        in_image = cv.convertScaleAbs(in_image, alpha = (255.0))\n",
        "        cv2.imwrite((f\"{self.root}/{name}.jpg\"), in_image)\n",
        "        in_json = {}\n",
        "        in_json = {'marks': value['marks'], 'slots': value['slots']}\n",
        "        path = f\"{self.root}/{name}.json\"\n",
        "        json.dump(in_json, codecs.open(path, 'w', encoding = 'utf-8'),\n",
        "                  separators = (',', ':'), sort_keys = True)  ### this saves the array in .json format"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXc54YqONUj3"
      },
      "source": [
        "## Rescale"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2tn6LFeNUj4"
      },
      "source": [
        "class Rescale(object):\n",
        "    \"\"\"Rescale the image in a sample to a given size.\n",
        "\n",
        "    Args:\n",
        "        output_size (tuple or int): Desired output size. If tuple, output is\n",
        "            matched to output_size. If int, smaller of image edges is matched\n",
        "            to output_size keeping aspect ratio the same.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image =sample['image']\n",
        "        marking_points = sample['marks']\n",
        "        slots = sample['slots']\n",
        "\n",
        "        h, w = image.shape[1:3]\n",
        "        if isinstance(self.output_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = self.output_size * h / w, self.output_size\n",
        "            else:\n",
        "                new_h, new_w = self.output_size, self.output_size * w / h\n",
        "        else:\n",
        "            new_h, new_w = self.output_size\n",
        "\n",
        "        new_h, new_w = int(new_h), int(new_w)\n",
        "        \n",
        "        img = image.permute(1, 2, 0)\n",
        "        img = transform.resize(img, (new_h, new_w))\n",
        "        to_Tensor = ToTensor()\n",
        "        img = to_Tensor(img)\n",
        "        #img = img.permute(1, 2, 0)\n",
        "        \n",
        "        mult = ([(new_h / h) ,(new_w / w) ,(new_h / h) ,(new_w / w)])\n",
        "        for i in range(len(marking_points)):\n",
        "            iterable_mp = list(marking_points[i][0:4])\n",
        "            for j in range(4):\n",
        "                iterable_mp[j] = (iterable_mp[j] * mult[j])\n",
        "            iterable_mp.append(marking_points[i][-1])    \n",
        "            marking_points[i] = MarkingPoint(*iterable_mp)    \n",
        "        return {'image': img,'marks': marking_points,'slots': slots}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NT_8dkWpI2uD"
      },
      "source": [
        "##load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfxszFUdz1xZ"
      },
      "source": [
        "def load(path,ls = []):\n",
        "  try:\n",
        "    park_dataset1 = ParkingSlotDataset(path,ls)\n",
        "    return park_dataset1\n",
        "  except:\n",
        "    return load(path)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHHu2sGiDU4k"
      },
      "source": [
        "dataset_path = r\"/content/drive/MyDrive/train_aug_all\" # train_20_mix # train_two_hundred_uni # training_mix2\n",
        "start = timeit.default_timer()\n",
        "park_dataset = load(dataset_path)\n",
        "stop = timeit.default_timer()\n",
        "park_dataset.split()\n",
        "print(\"\\n\\n\")\n",
        "print(\"Total Len is:\",len(park_dataset.all_samples))\n",
        "print(\"Train Len is:\",len(park_dataset.sample_names))\n",
        "print(\"Test Len is:\",len(park_dataset.test_names))\n",
        "print(\"Loading Time:\",stop-start)\n",
        "if park_dataset.sample_names.count(\"582/P_R_p2_img23_2076_aug_30\") == 1:\n",
        "  park_dataset.sample_names.remove(\"582/P_R_p2_img23_2076_aug_30\")\n",
        "send_notification()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc4lIMlS9-rf"
      },
      "source": [
        "# park_dataset.change_mode(\"train\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_cXaZCBNUj7"
      },
      "source": [
        "## - Dataset Preprocessing\n",
        "\n",
        "### + Image/Data rescaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpA6It9TNUj8"
      },
      "source": [
        "# image_rescaler = Rescale((512, 512))\n",
        "# for i in  range(len(park_dataset)):\n",
        "#      if park_dataset[i]['image'].shape[1] != 512:\n",
        "#          temp = image_rescaler(park_dataset[i])\n",
        "#          park_dataset.__set__(i,temp)       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kirUGjkdiWz6"
      },
      "source": [
        "## Collate Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77JRX9ooNUj9"
      },
      "source": [
        "def collate_mod(data):\n",
        "\n",
        "    d={}\n",
        "    \"\"\"\n",
        "       data: is a list of tuples with (example, label, length)\n",
        "             where 'example' is a tensor of arbitrary shape\n",
        "             and label/length are scalars\n",
        "    \"\"\"\n",
        "    #assert all('slots' in x for x in data)\n",
        "    #assert all('marks' in x for x in data)\n",
        "    #_, labels, lengths = (assert all('sentences' in x for x in batch))\n",
        "    #print(labels,lengths)\n",
        "    lengths_marks = []\n",
        "    lengths_slots = []\n",
        "    for i in range(len(data)):\n",
        "      \n",
        "      l1 =len(data[i]['marks'])\n",
        "      l2 =len(data[i]['slots'])\n",
        "      lengths_marks.append(l1)\n",
        "      lengths_slots.append(l2)\n",
        "\n",
        "    max_len_marks = max(lengths_marks)\n",
        "    max_len_slots = max(lengths_slots)\n",
        "\n",
        "    #n_ftrs = data[0][0].size(1)\n",
        "    features_marks = torch.zeros(len(data),max_len_marks,5)\n",
        "    features_slots = torch.zeros(len(data),max_len_slots,4)\n",
        "    \n",
        "\n",
        "    dict_im = torch.empty(len(data),3,512,512)\n",
        "    for i in range(len(data)):\n",
        "        j = len(data[i]['slots'])\n",
        "        t =torch.zeros(max_len_slots-j,4)\n",
        "        #print(torch.cat([(torch.Tensor(data[i]['slots'])), t]).shape)\n",
        "        features_slots[i] = torch.cat([(torch.Tensor(data[i]['slots'])), t])\n",
        "\n",
        "\n",
        "        k = len(data[i]['marks'])\n",
        "        t =torch.zeros(max_len_marks - k,5)\n",
        "        features_marks[i] = torch.cat([(torch.Tensor(data[i]['marks'])), t])\n",
        "\n",
        "        dict_im[i]=data[i]['image']\n",
        "\n",
        "    features_marks = features_marks.permute(1,2,0)\n",
        "    features_slots =features_slots.permute(1,2,0)\n",
        "    d= {'image' :dict_im, 'marks':features_marks,'slots':features_slots}\n",
        "    #return features_marks.float(),features_slots.float()\n",
        "\n",
        "   \n",
        "    return d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix586SdfNUj_"
      },
      "source": [
        "## Grid locator Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0YFN6SrNUj_"
      },
      "source": [
        "def locate_grid_mult(x,y):\n",
        "    if torch.sum(torch.less(x,0)):\n",
        "        x[x<0] = 0\n",
        "    if torch.sum(torch.less(y,0)):\n",
        "        y[y<0] = 0\n",
        "    if torch.sum(torch.greater(x,512)):\n",
        "        x[x>512] = 512\n",
        "    if torch.sum(torch.greater(y,512)):\n",
        "        y[y>512] = 512\n",
        "    grid_x = torch.divide(x,32).type(torch.int)\n",
        "    grid_y = torch.divide(y,32).type(torch.int)\n",
        "    grid_x[grid_x == 16] = 15\n",
        "    grid_y[grid_y == 16] = 15\n",
        "    \n",
        "    return grid_x,grid_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62qhTuX7NUj_"
      },
      "source": [
        "## Convert Marking Point to Tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFn6l7awNUkA"
      },
      "source": [
        "def marking_to_tensor(marking_points):\n",
        "    output_tensor = torch.zeros((5,marking_points[0].shape[0])) #5_features,#_training\n",
        "    for i in range(5):\n",
        "        output_tensor[i] = marking_points[i]\n",
        "    return output_tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UySlhTkSNUkA"
      },
      "source": [
        "## Complete 6x16x16 label Vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IW1Tf2_IfZEA"
      },
      "source": [
        "def calc_angle_2(p1,p2):\n",
        "  y_diff = p2[1]- p1[1]\n",
        "  x_diff = p2[0] - p1[0]\n",
        "  theta = np.zeros_like(x_diff)\n",
        "  theta = np.arctan2(y_diff,x_diff)\n",
        "  return theta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9saVlWJNUkA"
      },
      "source": [
        "def complete_marking_vector_label_mult(training_examples):\n",
        "\n",
        "    number_of_trainig_examples = training_examples['image'].shape[0]\n",
        "    label_vector = torch.zeros((number_of_trainig_examples,6,16,16),dtype=torch.float64)\n",
        "\n",
        "    label_vector[:,0,:,:] = label_vector[:,0,:,:] - 0.5\n",
        "    \n",
        "    for i in range(len(training_examples['marks'])):\n",
        "        try:\n",
        "          grid_x , grid_y = locate_grid_mult((training_examples['marks'][i][0]),\n",
        "                                            (training_examples['marks'][i][1]))\n",
        "          for j in range(number_of_trainig_examples):\n",
        "            label_vector[j,1:,grid_x[j],grid_y[j]] = (marking_to_tensor(training_examples['marks'][i]))[:,j]\n",
        "            label_vector[j,0,grid_x[j],grid_y[j]] = (0.5)  # confidence\n",
        "        except:\n",
        "          print(\"Problem at :\" ,i,\"\\n\\n\",training_examples[i])\n",
        "          with open(r\"/content/drive/MyDrive/trouble_list.pkl\", \"wb\") as fp:   #Pickling\n",
        "            pickle.dump(training_examples, fp)\n",
        "          with open(r\"/content/drive/MyDrive/trouble_index.pkl\", \"wb\") as fp:   #Pickling\n",
        "            pickle.dump(i, fp)   \n",
        "\n",
        "    label_vector[:,5,:,:] = (label_vector[:,5,:,:]) - 0.5 # shape\n",
        "    \n",
        "    x_old = torch.clone(label_vector[:,1,:,:])\n",
        "    y_old = torch.clone(label_vector[:,2,:,:])\n",
        "    x_val = (torch.clone(label_vector[:,1,:,:]) % 32) - 16\n",
        "    y_val = (torch.clone(label_vector[:,2,:,:]) % 32) - 16\n",
        "    dir_x = torch.clone(label_vector[:,3,:,:]) - (x_old - x_val)\n",
        "    dir_y = torch.clone(label_vector[:,4,:,:]) - (y_old - y_val)\n",
        "\n",
        "    marking_direction =  (calc_angle_2([x_val,y_val],[dir_x,dir_y]))\n",
        "    cos_theta = torch.cos(marking_direction) #* 16 # just to be in same range (dont forget to divide) # cos\n",
        "    sin_theta = torch.sin(marking_direction) #* 16 # just to be in same range (dont forget to divide) # sin\n",
        "  \n",
        "    label_vector[:,1,:,:] = ((label_vector[:,1,:,:] % 32) - 16)/32 # x\n",
        "    label_vector[:,2,:,:] = ((label_vector[:,2,:,:] % 32) - 16)/32 # y\n",
        "    label_vector[:,3,:,:] = torch.clone(cos_theta) * 0.5\n",
        "    label_vector[:,4,:,:] = torch.clone(sin_theta) * 0.5 # check here law feh arkam btt7at ll false\n",
        "    #label_vector = label_vector/10 \n",
        "\n",
        "    return (label_vector)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNsJOkR-NUkA"
      },
      "source": [
        "## Image Visualizer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nJEmuR7NUkB"
      },
      "source": [
        "def image_visualizer(parking_image):\n",
        "    plt.imshow(parking_image['image'].permute(1, 2, 0))\n",
        "    for i in range(len(parking_image['marks'])):\n",
        "        plt.plot([parking_image['marks'][i][0],parking_image['marks'][i][2]],[parking_image['marks'][i][1],parking_image['marks'][i][3]],'o')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjEB7GwqihH6"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "me7UT4O5moV8"
      },
      "source": [
        "def eval(output, label):\n",
        "    conf_flag = (label[:, 0, :, :] == 0.5)\n",
        "    not_conf_flag = (label[:, 0, :, :] == -0.5)\n",
        "    out = output.permute(0, 2, 3, 1)\n",
        "    label_p = label.permute(0, 2, 3, 1)\n",
        "    threhold_conf = 0.4\n",
        "    threhold_coordinates = 0.3125\n",
        "    therhold_direction_sin = 0.3\n",
        "    therhold_direction_cos = 0.92\n",
        "\n",
        "    true_negative = torch.sum(torch.bitwise_not(\n",
        "        torch.bitwise_xor((out[not_conf_flag][:, 0] < threhold_conf), label_p[not_conf_flag][:, 0] == -0.5)).type(\n",
        "        torch.float)).item()\n",
        "\n",
        "    false_negative = torch.sum(\n",
        "        (torch.bitwise_xor((out[not_conf_flag][:, 0] < threhold_conf), label_p[not_conf_flag][:, 0] == -0.5)).type(\n",
        "            torch.float)).item()\n",
        "\n",
        "    confidence_ones = torch.bitwise_not(\n",
        "        torch.bitwise_xor((out[conf_flag][:, 0] >= threhold_conf), label_p[conf_flag][:, 0] == 0.5))\n",
        "    x_val = (torch.abs(out[conf_flag][:, 1] - label_p[conf_flag][:, 1]) <= threhold_coordinates)\n",
        "    y_val = (torch.abs(out[conf_flag][:, 2] - label_p[conf_flag][:, 2]) <= threhold_coordinates)\n",
        "    x_dir_val = (torch.abs(out[conf_flag][:, 3] - label_p[conf_flag][:, 3]) <= therhold_direction_cos)\n",
        "    y_dir_val = (torch.abs(out[conf_flag][:, 4] - label_p[conf_flag][:, 4]) <= therhold_direction_sin)\n",
        "    shape_val = torch.bitwise_not(torch.bitwise_xor((out[conf_flag][:, 5] >= -0.05), label_p[conf_flag][:, 5] == 0.5))\n",
        "\n",
        "    total_positive = (label_p[conf_flag][:, 0]).shape[0]\n",
        "\n",
        "    c = torch.sum(confidence_ones).item()\n",
        "    x = torch.sum(x_val).item()\n",
        "    y = torch.sum(y_val).item()\n",
        "    dx = torch.sum(x_dir_val).item()\n",
        "    dy = torch.sum(y_dir_val).item()\n",
        "    s = torch.sum(shape_val).item()\n",
        "\n",
        "    # print(\"total positive:\",total_positive)\n",
        "    # print(\"conf:\",c)\n",
        "    # print(\"x:\",x)\n",
        "    # print(\"y:\",y)\n",
        "    # print(\"dx:\",dx)\n",
        "    # print(\"dy:\",dy)\n",
        "    # print(\"shape:\",s)\n",
        "\n",
        "    true_positive = torch.sum(torch.bitwise_and(confidence_ones, torch.bitwise_and\n",
        "    (x_val, torch.bitwise_and(y_val, torch.bitwise_and(x_dir_val, torch.bitwise_and(y_dir_val, shape_val))))).type(\n",
        "        torch.float)).item()\n",
        "\n",
        "    false_positive = (total_positive - true_positive)\n",
        "\n",
        "    accuracy = (true_negative + true_positive) / (true_negative + false_negative + true_positive + false_positive) if (\n",
        "                                                                                                                                  true_negative + false_negative + true_positive + false_positive) != 0 else 0\n",
        "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) != 0 else 0\n",
        "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) != 0 else 0\n",
        "    f1_score = 2 * (recall * precision) / (recall + precision) if (recall + precision) != 0 else 0\n",
        "    print(\"TP:\", true_positive, \" TN:\", true_negative, \" FP:\", false_positive, \" FN:\", false_negative)\n",
        "    return accuracy, f1_score , precision , recall\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFHQ5dZVGBik"
      },
      "source": [
        "## Compare weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyY5MoPSFyhO"
      },
      "source": [
        "#### 350"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9wBfhUEdwKM",
        "outputId": "dd3800e6-4ad3-4ed2-86e2-6929a867bfae"
      },
      "source": [
        "# model = ConvNet().to(device)\n",
        "# model = model.to(device) \n",
        "# checkpoint = torch.load(r'/content/drive/MyDrive/350k_after_shape_cp1_v11') # 250k_after_shape_cp1_v5 # 100k_after_shape_cp1_v15 # 350k_after_shape_cp1_v3\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa4TcBN1F3vA"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvasZW-tplT_"
      },
      "source": [
        "# park_dataset.change_mode(\"train\")\n",
        "# data_loader = DataLoader(park_dataset,\n",
        "#                              batch_size=32, shuffle=False , num_workers=4,collate_fn=collate_mod,pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wDera0hcmoV-",
        "outputId": "973de57d-b7e6-4b2a-c47b-81b56e8c2135"
      },
      "source": [
        "# model = model.eval()\n",
        "# accum_f1 = 0\n",
        "# accum_accu = 0\n",
        "# accum_prec = 0\n",
        "# accum_rec = 0\n",
        "# ctr = 0\n",
        "# with torch.no_grad():\n",
        "#   for i_batch , train_batch in enumerate(data_loader):\n",
        "#       images = train_batch['image'].to(device)\n",
        "#       labels = complete_marking_vector_label_mult(train_batch).to(device)\n",
        "#       outputs = model(images).to(device)\n",
        "#       outputs = outputs.reshape((-1,6,16,16))\n",
        "#       accuracy , evaluation , precision , recall = eval(outputs.float(), labels.float())\n",
        "#       accum_f1 += evaluation\n",
        "#       accum_accu += accuracy\n",
        "#       accum_prec += precision\n",
        "#       accum_rec += recall\n",
        "#       ctr += 1\n",
        "#       print(\"Current F1 score is:\",(accum_f1/ctr)*100,\"\\nCurrent Accuracy is:\",(accum_accu/ctr)*100,\"\\nCurrent Precision is:\",(accum_prec/ctr)*100,\"\\nCurrent Recall is:\",(accum_rec/ctr)*100,\"\\n\\n\")\n",
        "# print(\"Final F1 score is:\",(accum_f1/ctr)*100,\"\\nFinal Accuracy is:\",(accum_accu/ctr)*100,\"\\nFinal Precision is:\",(accum_prec/ctr)*100,\"\\nFinal Recall is:\",(accum_rec/ctr)*100,\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TP: 76.0  TN: 8088.0  FP: 14.0  FN: 14.0\n",
            "Current F1 score is: 84.44444444444444 \n",
            "Current Accuracy is: 99.658203125 \n",
            "Current Precision is: 84.44444444444444 \n",
            "Current Recall is: 84.44444444444444 \n",
            "\n",
            "\n",
            "TP: 77.0  TN: 8079.0  FP: 22.0  FN: 14.0\n",
            "Current F1 score is: 82.7485380116959 \n",
            "Current Accuracy is: 99.609375 \n",
            "Current Precision is: 81.11111111111111 \n",
            "Current Recall is: 84.52991452991454 \n",
            "\n",
            "\n",
            "TP: 72.0  TN: 8099.0  FP: 16.0  FN: 5.0\n",
            "Current F1 score is: 84.25660109870637 \n",
            "Current Accuracy is: 99.65413411458334 \n",
            "Current Precision is: 81.34680134680136 \n",
            "Current Recall is: 87.52210752210753 \n",
            "\n",
            "\n",
            "TP: 77.0  TN: 8089.0  FP: 14.0  FN: 12.0\n",
            "Current F1 score is: 84.58133971291866 \n",
            "Current Accuracy is: 99.6612548828125 \n",
            "Current Precision is: 82.16394716394717 \n",
            "Current Recall is: 87.27079412472672 \n",
            "\n",
            "\n",
            "TP: 72.0  TN: 8096.0  FP: 18.0  FN: 6.0\n",
            "Current F1 score is: 84.80792891319207 \n",
            "Current Accuracy is: 99.67041015625 \n",
            "Current Precision is: 81.73115773115774 \n",
            "Current Recall is: 88.27817376131983 \n",
            "\n",
            "\n",
            "TP: 77.0  TN: 8089.0  FP: 18.0  FN: 8.0\n",
            "Current F1 score is: 84.93253335358601 \n",
            "Current Accuracy is: 99.67244466145834 \n",
            "Current Precision is: 81.61807003912268 \n",
            "Current Recall is: 88.66318401678615 \n",
            "\n",
            "\n",
            "TP: 76.0  TN: 8094.0  FP: 14.0  FN: 8.0\n",
            "Current F1 score is: 85.27878885151378 \n",
            "Current Accuracy is: 99.68087332589286 \n",
            "Current Precision is: 82.0218378113115 \n",
            "Current Recall is: 88.9221849395582 \n",
            "\n",
            "\n",
            "TP: 69.0  TN: 8103.0  FP: 13.0  FN: 7.0\n",
            "Current F1 score is: 85.5366617640619 \n",
            "Current Accuracy is: 99.69024658203125 \n",
            "Current Precision is: 82.2874007678244 \n",
            "Current Recall is: 89.15559603263974 \n",
            "\n",
            "\n",
            "TP: 72.0  TN: 8096.0  FP: 15.0  FN: 9.0\n",
            "Current F1 score is: 85.5563977585312 \n",
            "Current Accuracy is: 99.69211154513889 \n",
            "Current Precision is: 82.33975853691669 \n",
            "Current Recall is: 89.12596190555632 \n",
            "\n",
            "\n",
            "TP: 80.0  TN: 8094.0  FP: 9.0  FN: 9.0\n",
            "Current F1 score is: 85.98952202762192 \n",
            "Current Accuracy is: 99.700927734375 \n",
            "Current Precision is: 83.09454672816885 \n",
            "Current Recall is: 89.2021297599445 \n",
            "\n",
            "\n",
            "TP: 70.0  TN: 8098.0  FP: 13.0  FN: 11.0\n",
            "Current F1 score is: 85.93282490315961 \n",
            "Current Accuracy is: 99.70148259943183 \n",
            "Current Precision is: 83.20752878902536 \n",
            "Current Recall is: 88.94918642598772 \n",
            "\n",
            "\n",
            "TP: 77.0  TN: 8082.0  FP: 22.0  FN: 11.0\n",
            "Current F1 score is: 85.63450125926887 \n",
            "Current Accuracy is: 99.69278971354166 \n",
            "Current Precision is: 82.75504953808806 \n",
            "Current Recall is: 88.82842089048873 \n",
            "\n",
            "\n",
            "TP: 69.0  TN: 8099.0  FP: 16.0  FN: 8.0\n",
            "Current F1 score is: 85.59993848433935 \n",
            "Current Accuracy is: 99.69388521634616 \n",
            "Current Precision is: 82.63362038809939 \n",
            "Current Recall is: 88.88857233048111 \n",
            "\n",
            "\n",
            "TP: 66.0  TN: 8096.0  FP: 21.0  FN: 9.0\n",
            "Current F1 score is: 85.30576298413521 \n",
            "Current Accuracy is: 99.68959263392857 \n",
            "Current Precision is: 82.14993814362924 \n",
            "Current Recall is: 88.82510287830388 \n",
            "\n",
            "\n",
            "TP: 74.0  TN: 8077.0  FP: 26.0  FN: 15.0\n",
            "Current F1 score is: 84.83917067231809 \n",
            "Current Accuracy is: 99.67692057291667 \n",
            "Current Precision is: 81.60660893405395 \n",
            "Current Recall is: 88.44650051413232 \n",
            "\n",
            "\n",
            "TP: 74.0  TN: 8096.0  FP: 12.0  FN: 10.0\n",
            "Current F1 score is: 84.97789897588645 \n",
            "Current Accuracy is: 99.68032836914062 \n",
            "Current Precision is: 81.88410285241977 \n",
            "Current Recall is: 88.42454661295143 \n",
            "\n",
            "\n",
            "error index is: 557\n",
            "error here 419/P_R_image20160725142318_612_aug_345\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-60ccdf983316>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mctr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrain_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m       \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomplete_marking_vector_label_mult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "error index is: 620\n",
            "error here 8/P_R_2081_aug_0\n",
            "error index is: 524\n",
            "error here 144/caro-par1006_aug_345\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hzl_DDUF-bV"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fmci4q6TFvZc",
        "outputId": "64bd28bb-09d5-4df1-adae-6331763c641c"
      },
      "source": [
        "# park_dataset.change_mode(\"test\")\n",
        "# data_loader = DataLoader(park_dataset,\n",
        "#                              batch_size=32, shuffle=False , num_workers=4,collate_fn=collate_mod,pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing... \n",
            "Testing Dataset length is: 54644\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mSCCN37yFvZd",
        "outputId": "36d6524e-3fe4-49e5-89cc-97a6c1405495"
      },
      "source": [
        "# model = model.eval()\n",
        "# accum_f1 = 0\n",
        "# accum_accu = 0\n",
        "# accum_prec = 0\n",
        "# accum_rec = 0\n",
        "# ctr = 0\n",
        "# with torch.no_grad():\n",
        "#   for i_batch , train_batch in enumerate(data_loader):\n",
        "#       images = train_batch['image'].to(device)\n",
        "#       labels = complete_marking_vector_label_mult(train_batch).to(device)\n",
        "#       outputs = model(images).to(device)\n",
        "#       outputs = outputs.reshape((-1,6,16,16))\n",
        "#       accuracy , evaluation , precision , recall = eval(outputs.float(), labels.float())\n",
        "#       accum_f1 += evaluation\n",
        "#       accum_accu += accuracy\n",
        "#       accum_prec += precision\n",
        "#       accum_rec += recall\n",
        "#       ctr += 1\n",
        "#       print(\"Current F1 score is:\",(accum_f1/ctr)*100,\"\\nCurrent Accuracy is:\",(accum_accu/ctr)*100,\"\\nCurrent Precision is:\",(accum_prec/ctr)*100,\"\\nCurrent Recall is:\",(accum_rec/ctr)*100,\"\\n\\n\")\n",
        "# print(\"Final F1 score is:\",(accum_f1/ctr)*100,\"\\nFinal Accuracy is:\",(accum_accu/ctr)*100,\"\\nFinal Precision is:\",(accum_prec/ctr)*100,\"\\nFinal Recall is:\",(accum_rec/ctr)*100,\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TP: 76.0  TN: 8089.0  FP: 18.0  FN: 9.0\n",
            "Current F1 score is: 84.91620111731844 \n",
            "Current Accuracy is: 99.67041015625 \n",
            "Current Precision is: 80.85106382978722 \n",
            "Current Recall is: 89.41176470588236 \n",
            "\n",
            "\n",
            "TP: 82.0  TN: 8085.0  FP: 16.0  FN: 9.0\n",
            "Current F1 score is: 85.8443439449026 \n",
            "Current Accuracy is: 99.6826171875 \n",
            "Current Precision is: 82.26226660877116 \n",
            "Current Recall is: 89.76082740788624 \n",
            "\n",
            "\n",
            "TP: 69.0  TN: 8098.0  FP: 14.0  FN: 11.0\n",
            "Current F1 score is: 85.45042152564059 \n",
            "Current Accuracy is: 99.68668619791666 \n",
            "Current Precision is: 82.5523544460081 \n",
            "Current Recall is: 88.59055160525749 \n",
            "\n",
            "\n",
            "TP: 77.0  TN: 8088.0  FP: 20.0  FN: 7.0\n",
            "Current F1 score is: 85.35853437627465 \n",
            "Current Accuracy is: 99.6826171875 \n",
            "Current Precision is: 81.75962665924833 \n",
            "Current Recall is: 89.35958037060978 \n",
            "\n",
            "\n",
            "TP: 72.0  TN: 8090.0  FP: 20.0  FN: 10.0\n",
            "Current F1 score is: 84.83855163895075 \n",
            "Current Accuracy is: 99.6728515625 \n",
            "Current Precision is: 81.05987524044215 \n",
            "Current Recall is: 89.04863990624392 \n",
            "\n",
            "\n",
            "TP: 72.0  TN: 8093.0  FP: 15.0  FN: 12.0\n",
            "Current F1 score is: 84.73388075175721 \n",
            "Current Accuracy is: 99.67244466145834 \n",
            "Current Precision is: 81.34299948197766 \n",
            "Current Recall is: 88.49291420758422 \n",
            "\n",
            "\n",
            "TP: 75.0  TN: 8092.0  FP: 17.0  FN: 8.0\n",
            "Current F1 score is: 84.87393860354699 \n",
            "Current Accuracy is: 99.67564174107143 \n",
            "Current Precision is: 81.36853371747152 \n",
            "Current Recall is: 88.75984728980539 \n",
            "\n",
            "\n",
            "TP: 75.0  TN: 8094.0  FP: 14.0  FN: 9.0\n",
            "Current F1 score is: 85.10284656712096 \n",
            "Current Accuracy is: 99.68109130859375 \n",
            "Current Precision is: 81.73117486795613 \n",
            "Current Recall is: 88.82558066429401 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-60ccdf983316>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mctr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrain_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m       \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomplete_marking_vector_label_mult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "error index is: 342\n",
            "error here 711/P_R_p2_img58_0294_aug_165\n",
            "error index is: 277\n",
            "error here 10/P_R_2199_aug_195\n",
            "error index is: 374\n",
            "error here 53/P_R_1764_aug_135\n",
            "error index is: 308\n",
            "error here 361/ahm-par2299_aug_0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1WXWsA5GKIs"
      },
      "source": [
        "### 350 Variant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ho7XaIlHGTqw",
        "outputId": "5e806147-a2d7-4608-b5ca-ad864bb78490"
      },
      "source": [
        "# model = ConvNet().to(device)\n",
        "# model = model.to(device) \n",
        "# checkpoint = torch.load(r'/content/drive/MyDrive/variant_350k_after_shape_cp1_v8') # 250k_after_shape_cp1_v5 # 100k_after_shape_cp1_v15 # 350k_after_shape_cp1_v3\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4QMghW8GTqx"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9donuGzGTqx",
        "outputId": "8bd269f5-9f20-4e5f-924e-ebf685533d85"
      },
      "source": [
        "# park_dataset.change_mode(\"train\")\n",
        "# data_loader = DataLoader(park_dataset,\n",
        "#                              batch_size=32, shuffle=False , num_workers=4,collate_fn=collate_mod,pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training... \n",
            "Training Dataset length is: 309643\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OM_p5GXZGTqx",
        "outputId": "0eb8e950-fce0-4de5-95e8-25a07eb94e76"
      },
      "source": [
        "# model = model.eval()\n",
        "# accum_f1 = 0\n",
        "# accum_accu = 0\n",
        "# accum_prec = 0\n",
        "# accum_rec = 0\n",
        "# ctr = 0\n",
        "# with torch.no_grad():\n",
        "#   for i_batch , train_batch in enumerate(data_loader):\n",
        "#       images = train_batch['image'].to(device)\n",
        "#       labels = complete_marking_vector_label_mult(train_batch).to(device)\n",
        "#       outputs = model(images).to(device)\n",
        "#       outputs = outputs.reshape((-1,6,16,16))\n",
        "#       accuracy , evaluation , precision , recall = eval(outputs.float(), labels.float())\n",
        "#       accum_f1 += evaluation\n",
        "#       accum_accu += accuracy\n",
        "#       accum_prec += precision\n",
        "#       accum_rec += recall\n",
        "#       ctr += 1\n",
        "#       print(\"Current F1 score is:\",(accum_f1/ctr)*100,\"\\nCurrent Accuracy is:\",(accum_accu/ctr)*100,\"\\nCurrent Precision is:\",(accum_prec/ctr)*100,\"\\nCurrent Recall is:\",(accum_rec/ctr)*100,\"\\n\\n\")\n",
        "# print(\"Final F1 score is:\",(accum_f1/ctr)*100,\"\\nFinal Accuracy is:\",(accum_accu/ctr)*100,\"\\nFinal Precision is:\",(accum_prec/ctr)*100,\"\\nFinal Recall is:\",(accum_rec/ctr)*100,\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TP: 78.0  TN: 8088.0  FP: 12.0  FN: 14.0\n",
            "Current F1 score is: 85.71428571428571 \n",
            "Current Accuracy is: 99.6826171875 \n",
            "Current Precision is: 86.66666666666667 \n",
            "Current Recall is: 84.78260869565217 \n",
            "\n",
            "\n",
            "TP: 77.0  TN: 8079.0  FP: 22.0  FN: 14.0\n",
            "Current F1 score is: 83.38345864661653 \n",
            "Current Accuracy is: 99.62158203125 \n",
            "Current Precision is: 82.22222222222221 \n",
            "Current Recall is: 84.69899665551839 \n",
            "\n",
            "\n",
            "TP: 74.0  TN: 8100.0  FP: 14.0  FN: 4.0\n",
            "Current F1 score is: 85.30784793308572 \n",
            "Current Accuracy is: 99.67447916666666 \n",
            "Current Precision is: 82.84511784511784 \n",
            "Current Recall is: 88.0899293942772 \n",
            "\n",
            "\n",
            "TP: 79.0  TN: 8090.0  FP: 12.0  FN: 11.0\n",
            "Current F1 score is: 85.8040903697038 \n",
            "Current Accuracy is: 99.6856689453125 \n",
            "Current Precision is: 83.83713508713508 \n",
            "Current Recall is: 88.01189149015235 \n",
            "\n",
            "\n",
            "TP: 72.0  TN: 8096.0  FP: 18.0  FN: 6.0\n",
            "Current F1 score is: 85.78612943862018 \n",
            "Current Accuracy is: 99.68994140625 \n",
            "Current Precision is: 83.06970806970806 \n",
            "Current Recall is: 88.87105165366034 \n",
            "\n",
            "\n",
            "TP: 78.0  TN: 8087.0  FP: 17.0  FN: 10.0\n",
            "Current F1 score is: 85.69609147207419 \n",
            "Current Accuracy is: 99.68668619791666 \n",
            "Current Precision is: 82.90896725107251 \n",
            "Current Recall is: 88.8319369841109 \n",
            "\n",
            "\n",
            "TP: 77.0  TN: 8094.0  FP: 13.0  FN: 8.0\n",
            "Current F1 score is: 86.02522126177787 \n",
            "Current Accuracy is: 99.69482421875 \n",
            "Current Precision is: 83.28705129457008 \n",
            "Current Recall is: 89.0828367426833 \n",
            "\n",
            "\n",
            "TP: 68.0  TN: 8102.0  FP: 14.0  FN: 8.0\n",
            "Current F1 score is: 86.03156227494172 \n",
            "Current Accuracy is: 99.69940185546875 \n",
            "Current Precision is: 83.2420235412854 \n",
            "Current Recall is: 89.13169267616367 \n",
            "\n",
            "\n",
            "TP: 71.0  TN: 8095.0  FP: 16.0  FN: 10.0\n",
            "Current F1 score is: 85.86403419148259 \n",
            "Current Accuracy is: 99.69753689236111 \n",
            "Current Precision is: 83.06059819250913 \n",
            "Current Recall is: 88.9675402663293 \n",
            "\n",
            "\n",
            "TP: 81.0  TN: 8092.0  FP: 8.0  FN: 11.0\n",
            "Current F1 score is: 86.22790701542824 \n",
            "Current Accuracy is: 99.70458984375 \n",
            "Current Precision is: 83.85566196876383 \n",
            "Current Recall is: 88.87513406578333 \n",
            "\n",
            "\n",
            "TP: 68.0  TN: 8098.0  FP: 15.0  FN: 11.0\n",
            "Current F1 score is: 86.020880676203 \n",
            "Current Accuracy is: 99.70259232954545 \n",
            "Current Precision is: 83.68039368463744 \n",
            "Current Recall is: 88.62066272953834 \n",
            "\n",
            "\n",
            "TP: 79.0  TN: 8080.0  FP: 20.0  FN: 13.0\n",
            "Current F1 score is: 85.74601671060319 \n",
            "Current Accuracy is: 99.69380696614584 \n",
            "Current Precision is: 83.35685919408262 \n",
            "Current Recall is: 88.39140460352608 \n",
            "\n",
            "\n",
            "TP: 70.0  TN: 8098.0  FP: 15.0  FN: 9.0\n",
            "Current F1 score is: 85.71677339890577 \n",
            "Current Accuracy is: 99.69482421875 \n",
            "Current Precision is: 83.27963473118939 \n",
            "Current Recall is: 88.40803462915552 \n",
            "\n",
            "\n",
            "TP: 66.0  TN: 8095.0  FP: 21.0  FN: 10.0\n",
            "Current F1 score is: 85.37854637698568 \n",
            "Current Accuracy is: 99.68959263392857 \n",
            "Current Precision is: 82.74980860506996 \n",
            "Current Recall is: 88.29618253158425 \n",
            "\n",
            "\n",
            "TP: 73.0  TN: 8076.0  FP: 27.0  FN: 16.0\n",
            "Current F1 score is: 84.83655510176511 \n",
            "Current Accuracy is: 99.67529296875 \n",
            "Current Precision is: 82.09982136473197 \n",
            "Current Recall is: 87.87793515681946 \n",
            "\n",
            "\n",
            "TP: 73.0  TN: 8095.0  FP: 13.0  FN: 11.0\n",
            "Current F1 score is: 84.90191746672832 \n",
            "Current Accuracy is: 99.67727661132812 \n",
            "Current Precision is: 82.27381508757576 \n",
            "Current Recall is: 87.81711182856586 \n",
            "\n",
            "\n",
            "TP: 72.0  TN: 8107.0  FP: 9.0  FN: 4.0\n",
            "Current F1 score is: 85.3029661582696 \n",
            "Current Accuracy is: 99.68692555147058 \n",
            "Current Precision is: 82.6629370758883 \n",
            "Current Recall is: 88.22415478601864 \n",
            "\n",
            "\n",
            "TP: 78.0  TN: 8088.0  FP: 14.0  FN: 12.0\n",
            "Current F1 score is: 85.32581724471494 \n",
            "Current Accuracy is: 99.68668619791666 \n",
            "Current Precision is: 82.78069661031961 \n",
            "Current Recall is: 88.13762766827688 \n",
            "\n",
            "\n",
            "TP: 70.0  TN: 8087.0  FP: 24.0  FN: 11.0\n",
            "Current F1 score is: 85.04551107394047 \n",
            "Current Accuracy is: 99.68068976151315 \n",
            "Current Precision is: 82.34319074169137 \n",
            "Current Recall is: 88.04721321660018 \n",
            "\n",
            "\n",
            "TP: 85.0  TN: 8086.0  FP: 9.0  FN: 12.0\n",
            "Current F1 score is: 85.24349730034815 \n",
            "Current Accuracy is: 99.683837890625 \n",
            "Current Precision is: 82.74730780035149 \n",
            "Current Recall is: 88.02629585473925 \n",
            "\n",
            "\n",
            "TP: 73.0  TN: 8091.0  FP: 21.0  FN: 7.0\n",
            "Current F1 score is: 85.17990438018927 \n",
            "Current Accuracy is: 99.6826171875 \n",
            "Current Precision is: 82.50503478452929 \n",
            "Current Recall is: 88.17980557594215 \n",
            "\n",
            "\n",
            "TP: 68.0  TN: 8092.0  FP: 22.0  FN: 10.0\n",
            "Current F1 score is: 84.98774422437981 \n",
            "Current Accuracy is: 99.6792879971591 \n",
            "Current Precision is: 82.18914936503047 \n",
            "Current Recall is: 88.13433655792147 \n",
            "\n",
            "\n",
            "TP: 73.0  TN: 8093.0  FP: 17.0  FN: 9.0\n",
            "Current F1 score is: 84.98322147246036 \n",
            "Current Accuracy is: 99.67943274456522 \n",
            "Current Precision is: 82.1422781365992 \n",
            "Current Recall is: 88.17303454426848 \n",
            "\n",
            "\n",
            "TP: 74.0  TN: 8089.0  FP: 18.0  FN: 11.0\n",
            "Current F1 score is: 84.92624637815115 \n",
            "Current Accuracy is: 99.67803955078125 \n",
            "Current Precision is: 82.07113248960322 \n",
            "Current Recall is: 88.1266090853161 \n",
            "\n",
            "\n",
            "TP: 76.0  TN: 8079.0  FP: 28.0  FN: 9.0\n",
            "Current F1 score is: 84.74612773995632 \n",
            "Current Accuracy is: 99.6728515625 \n",
            "Current Precision is: 81.71136411309601 \n",
            "Current Recall is: 88.17801531013876 \n",
            "\n",
            "\n",
            "TP: 74.0  TN: 8097.0  FP: 13.0  FN: 8.0\n",
            "Current F1 score is: 84.8548906921519 \n",
            "Current Accuracy is: 99.67557466947116 \n",
            "Current Precision is: 81.84006054199108 \n",
            "Current Recall is: 88.25747250740359 \n",
            "\n",
            "\n",
            "TP: 54.0  TN: 8098.0  FP: 10.0  FN: 30.0\n",
            "Current F1 score is: 84.41481966551564 \n",
            "Current Accuracy is: 99.66950593171296 \n",
            "Current Precision is: 81.933947188584 \n",
            "Current Recall is: 87.36962961030399 \n",
            "\n",
            "\n",
            "TP: 68.0  TN: 8097.0  FP: 18.0  FN: 9.0\n",
            "Current F1 score is: 84.37984692110743 \n",
            "Current Accuracy is: 99.66953822544643 \n",
            "Current Precision is: 81.83165505477244 \n",
            "Current Recall is: 87.40327456392485 \n",
            "\n",
            "\n",
            "TP: 77.0  TN: 8091.0  FP: 14.0  FN: 10.0\n",
            "Current F1 score is: 84.45353681805491 \n",
            "Current Accuracy is: 99.67083108836206 \n",
            "Current Precision is: 81.92764572927632 \n",
            "Current Recall is: 87.44129085918388 \n",
            "\n",
            "\n",
            "TP: 74.0  TN: 8078.0  FP: 23.0  FN: 17.0\n",
            "Current F1 score is: 84.26253239929706 \n",
            "Current Accuracy is: 99.66552734375 \n",
            "Current Precision is: 81.73967953142758 \n",
            "Current Recall is: 87.23720387450047 \n",
            "\n",
            "\n",
            "TP: 76.0  TN: 8091.0  FP: 16.0  FN: 9.0\n",
            "Current F1 score is: 84.3145702643097 \n",
            "Current Accuracy is: 99.66647240423387 \n",
            "Current Precision is: 81.76771230951618 \n",
            "Current Recall is: 87.30735099809344 \n",
            "\n",
            "\n",
            "TP: 71.0  TN: 8094.0  FP: 17.0  FN: 10.0\n",
            "Current F1 score is: 84.30547958852044 \n",
            "Current Accuracy is: 99.66659545898438 \n",
            "Current Precision is: 81.73377811802563 \n",
            "Current Recall is: 87.31819381026722 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-60ccdf983316>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mctr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrain_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m       \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomplete_marking_vector_label_mult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "error index is: 1184\n",
            "error here 68/P_R_0540_aug_45\n",
            "error index is: 1091\n",
            "error here 317/gig-par536_aug_210\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-iXKwg3GTqx",
        "outputId": "000125ea-2539-4566-90ee-7574bb168e29"
      },
      "source": [
        "# park_dataset.change_mode(\"test\")\n",
        "# data_loader = DataLoader(park_dataset,\n",
        "#                              batch_size=32, shuffle=False , num_workers=4,collate_fn=collate_mod,pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing... \n",
            "Testing Dataset length is: 54644\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w9wSHsdeGTqy",
        "outputId": "dac9ea3d-08d1-41ed-ef18-a5aa5c698fa7"
      },
      "source": [
        "# model = model.eval()\n",
        "# accum_f1 = 0\n",
        "# accum_accu = 0\n",
        "# accum_prec = 0\n",
        "# accum_rec = 0\n",
        "# ctr = 0\n",
        "# with torch.no_grad():\n",
        "#   for i_batch , train_batch in enumerate(data_loader):\n",
        "#       images = train_batch['image'].to(device)\n",
        "#       labels = complete_marking_vector_label_mult(train_batch).to(device)\n",
        "#       outputs = model(images).to(device)\n",
        "#       outputs = outputs.reshape((-1,6,16,16))\n",
        "#       accuracy , evaluation , precision , recall = eval(outputs.float(), labels.float())\n",
        "#       accum_f1 += evaluation\n",
        "#       accum_accu += accuracy\n",
        "#       accum_prec += precision\n",
        "#       accum_rec += recall\n",
        "#       ctr += 1\n",
        "#       print(\"Current F1 score is:\",(accum_f1/ctr)*100,\"\\nCurrent Accuracy is:\",(accum_accu/ctr)*100,\"\\nCurrent Precision is:\",(accum_prec/ctr)*100,\"\\nCurrent Recall is:\",(accum_rec/ctr)*100,\"\\n\\n\")\n",
        "# print(\"Final F1 score is:\",(accum_f1/ctr)*100,\"\\nFinal Accuracy is:\",(accum_accu/ctr)*100,\"\\nFinal Precision is:\",(accum_prec/ctr)*100,\"\\nFinal Recall is:\",(accum_rec/ctr)*100,\"\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TP: 76.0  TN: 8088.0  FP: 18.0  FN: 10.0\n",
            "Current F1 score is: 84.44444444444444 \n",
            "Current Accuracy is: 99.658203125 \n",
            "Current Precision is: 80.85106382978722 \n",
            "Current Recall is: 88.37209302325581 \n",
            "\n",
            "\n",
            "TP: 82.0  TN: 8086.0  FP: 16.0  FN: 8.0\n",
            "Current F1 score is: 85.83924349881796 \n",
            "Current Accuracy is: 99.6826171875 \n",
            "Current Precision is: 82.26226660877116 \n",
            "Current Recall is: 89.74160206718346 \n",
            "\n",
            "\n",
            "TP: 71.0  TN: 8094.0  FP: 12.0  FN: 15.0\n",
            "Current F1 score is: 85.2340518788964 \n",
            "Current Accuracy is: 99.67854817708334 \n",
            "Current Precision is: 83.3555672974137 \n",
            "Current Recall is: 87.3471145564169 \n",
            "\n",
            "\n",
            "TP: 75.0  TN: 8088.0  FP: 22.0  FN: 7.0\n",
            "Current F1 score is: 84.87525957956336 \n",
            "Current Accuracy is: 99.67041015625 \n",
            "Current Precision is: 81.84657238027677 \n",
            "Current Recall is: 88.37618957584925 \n",
            "\n",
            "\n",
            "TP: 73.0  TN: 8089.0  FP: 19.0  FN: 11.0\n",
            "Current F1 score is: 84.49111675455978 \n",
            "Current Accuracy is: 99.6630859375 \n",
            "Current Precision is: 81.34682312161272 \n",
            "Current Recall is: 88.08190404163177 \n",
            "\n",
            "\n",
            "TP: 72.0  TN: 8093.0  FP: 15.0  FN: 12.0\n",
            "Current F1 score is: 84.4443516814314 \n",
            "Current Accuracy is: 99.664306640625 \n",
            "Current Precision is: 81.58212271628646 \n",
            "Current Recall is: 87.6873009870741 \n",
            "\n",
            "\n",
            "TP: 76.0  TN: 8091.0  FP: 16.0  FN: 9.0\n",
            "Current F1 score is: 84.64883090046823 \n",
            "Current Accuracy is: 99.66866629464286 \n",
            "Current Precision is: 81.72877599284182 \n",
            "Current Recall is: 87.93365294690385 \n",
            "\n",
            "\n",
            "TP: 75.0  TN: 8094.0  FP: 14.0  FN: 9.0\n",
            "Current F1 score is: 84.90587732692705 \n",
            "Current Accuracy is: 99.67498779296875 \n",
            "Current Precision is: 82.04638685890512 \n",
            "Current Recall is: 88.10266061425516 \n",
            "\n",
            "\n",
            "TP: 71.0  TN: 8089.0  FP: 24.0  FN: 8.0\n",
            "Current F1 score is: 84.5395793353017 \n",
            "Current Accuracy is: 99.66769748263889 \n",
            "Current Precision is: 81.23421521961158 \n",
            "Current Recall is: 88.29941140395114 \n",
            "\n",
            "\n",
            "TP: 72.0  TN: 8082.0  FP: 22.0  FN: 16.0\n",
            "Current F1 score is: 83.99770931385945 \n",
            "Current Accuracy is: 99.654541015625 \n",
            "Current Precision is: 80.77036816573553 \n",
            "Current Recall is: 87.65128844537419 \n",
            "\n",
            "\n",
            "TP: 84.0  TN: 8086.0  FP: 12.0  FN: 10.0\n",
            "Current F1 score is: 84.39983143365211 \n",
            "Current Accuracy is: 99.6615323153409 \n",
            "Current Precision is: 81.3821528779414 \n",
            "Current Recall is: 87.80678059830923 \n",
            "\n",
            "\n",
            "TP: 81.0  TN: 8088.0  FP: 19.0  FN: 4.0\n",
            "Current F1 score is: 84.66380944481175 \n",
            "Current Accuracy is: 99.66634114583334 \n",
            "Current Precision is: 81.35030680477962 \n",
            "Current Recall is: 88.4307253523717 \n",
            "\n",
            "\n",
            "TP: 80.0  TN: 8089.0  FP: 10.0  FN: 13.0\n",
            "Current F1 score is: 84.876723640524 \n",
            "Current Accuracy is: 99.67041015625 \n",
            "Current Precision is: 81.93019773432648 \n",
            "Current Recall is: 88.24540073883111 \n",
            "\n",
            "\n",
            "TP: 73.0  TN: 8084.0  FP: 22.0  FN: 13.0\n",
            "Current F1 score is: 84.5757421965876 \n",
            "Current Accuracy is: 99.66343470982143 \n",
            "Current Precision is: 81.56676255781444 \n",
            "Current Recall is: 88.00528075250264 \n",
            "\n",
            "\n",
            "TP: 77.0  TN: 8084.0  FP: 22.0  FN: 9.0\n",
            "Current F1 score is: 84.4869089330313 \n",
            "Current Accuracy is: 99.66064453125 \n",
            "Current Precision is: 81.31416357247868 \n",
            "Current Recall is: 88.10725428373115 \n",
            "\n",
            "\n",
            "TP: 74.0  TN: 8085.0  FP: 20.0  FN: 13.0\n",
            "Current F1 score is: 84.3169743622859 \n",
            "Current Accuracy is: 99.65667724609375 \n",
            "Current Precision is: 81.1522411151562 \n",
            "Current Recall is: 87.91664284502095 \n",
            "\n",
            "\n",
            "TP: 81.0  TN: 8078.0  FP: 19.0  FN: 14.0\n",
            "Current F1 score is: 84.24403016902927 \n",
            "Current Accuracy is: 99.65317670036765 \n",
            "Current Precision is: 81.14328575544113 \n",
            "Current Recall is: 87.76055549500424 \n",
            "\n",
            "\n",
            "TP: 72.0  TN: 8093.0  FP: 16.0  FN: 11.0\n",
            "Current F1 score is: 84.24216884384929 \n",
            "Current Accuracy is: 99.65413411458334 \n",
            "Current Precision is: 81.18077998114894 \n",
            "Current Recall is: 87.70424618704884 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-60ccdf983316>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mctr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtrain_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m       \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomplete_marking_vector_label_mult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1143\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "error index is: 742\n",
            "error here 734/P_R_p2_img73_0846_aug_330\n",
            "error index is: 604\n",
            "error here 480/P_R_img8_2109_aug_330\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3stIPzoHqmT"
      },
      "source": [
        "## Experimental Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L912ffeX8vIC",
        "outputId": "8315fe57-6bc7-46e2-d69d-cd056888d707"
      },
      "source": [
        "sat= 10\n",
        "initial =0.01\n",
        "inflection=50\n",
        "\n",
        "c=sat\n",
        "a= c/initial - 1\n",
        "b= np.log(a) / inflection\n",
        "print(c,b,a)\n",
        "\n",
        "def decay_fn(x):\n",
        "  #exp_term = torch.mul(x,inflection)\n",
        "  #exp = torch.exp(exp_term)\n",
        "  #exp_1 = torch.add(exp,1)\n",
        "  #inv = torch.pow(exp_1,-1)\n",
        "  #y = torch.mul(inv,sat)\n",
        "\n",
        "  exp_term = torch.mul(x,-1*b)\n",
        "  exp = torch.exp(exp_term)\n",
        "  exp_a = torch.mul(exp,a)\n",
        "  exp_1 = torch.add(exp_a,1)\n",
        "  inv = torch.pow(exp_1,-1)\n",
        "  y = torch.mul(inv,c)\n",
        "  #y = sat / (1+ initial * math.exp(-1*inflection*x[:]))\n",
        "  return y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 0.13813509557297107 999.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoSxddTKil_I"
      },
      "source": [
        "## Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q2KjNnhXp2P"
      },
      "source": [
        "def my_loss(output, label):\n",
        "  conf_flag = (label[:,0,:,:] == 0.5) # change to 100\n",
        "  not_conf_flag = (label[:,0,:,:] == -0.5)\n",
        "  out = output.permute(0,2,3,1)\n",
        "  la = label.permute(0,2,3,1)\n",
        "  Loss1 = torch.mean((out[conf_flag] - la[conf_flag])**2)     # kolo\n",
        "  Loss2 = torch.mean((out[not_conf_flag][:,0] - la[not_conf_flag][:,0])**2)  # confidence\n",
        "  return torch.abs(Loss1) + torch.abs(Loss2) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-zLKaUFzYL-"
      },
      "source": [
        "data_loader = DataLoader(park_dataset,\n",
        "                             batch_size=32, shuffle=True , num_workers=4,collate_fn=collate_mod,pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv9_pHUqiqa4"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq4jPLFyk7O4"
      },
      "source": [
        "# Hyper parameters\n",
        "num_epochs = 50000000 \n",
        "learning_rate = 0.001\n",
        "model = ConvNet().to(device)\n",
        "model.to(device) \n",
        "optimizer = optim.RAdam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "\n",
        "checkpoint = torch.load(r'/content/drive/MyDrive/variant_350k_after_shape_cp1_v8') # 20\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "myloss = checkpoint['loss']\n",
        "scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min',factor=0.9, patience=50, cooldown=20, min_lr=0, verbose=False)\n",
        "scheduler1 = (checkpoint['scheduler'])\n",
        "\n",
        "\n",
        "model = model.train();\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GabqeFksNKl2"
      },
      "source": [
        "# %reload_ext tensorboard\n",
        "# %tensorboard --logdir=/content/drive/MyDrive/variant_350k_after_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0XfagvxmY8Q"
      },
      "source": [
        "# scheduler1 = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr = 0.00000000001, max_lr=8e-4, step_size_up=6000, step_size_down=6000, mode='triangular', gamma=1, scale_fn=None, cycle_momentum=False, base_momentum=0.8, max_momentum=0.9, last_epoch=-1, verbose=False)\n",
        "#scheduler1 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min',factor=0.9, patience=40, cooldown=10, min_lr=0, verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKSkEfCqjetb"
      },
      "source": [
        "n = 8\n",
        "writer = SummaryWriter(f'/content/drive/MyDrive/variant_350k_after_shape/{n}') \n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=/content/drive/MyDrive/variant_350k_after_shape\n",
        "total_step = len(data_loader)\n",
        "graph_itrator = 0\n",
        "graph_itrator_2 = 0\n",
        "batch_loss = 0\n",
        "\n",
        "actual_batch_size = 24 * 32 # 8192\n",
        "BS = data_loader.batch_size\n",
        "actual_batch_factor = actual_batch_size/BS\n",
        "number_of_batches = math.ceil(len(park_dataset.sample_names)/BS)\n",
        "\n",
        "model = model.train();\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i_batch , train_batch in enumerate(data_loader):\n",
        "    images = train_batch['image'].to(device)\n",
        "    labels = complete_marking_vector_label_mult(train_batch).to(device)\n",
        "\n",
        "    # Forward pass        \n",
        "    outputs = model(images).to(device)\n",
        "    outputs = outputs.reshape((-1,6,16,16))      \n",
        "    myloss = my_loss(outputs.float(), labels.float())\n",
        "\n",
        "    # Backward and optimize        \n",
        "    myloss.backward()\n",
        "\n",
        "    batch_loss += myloss.item()\n",
        "\n",
        "    if graph_itrator % actual_batch_factor == 0:     \n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "      if graph_itrator > 10:\n",
        "        scheduler1.step(batch_loss)\n",
        "        s  = scheduler1\n",
        "        # if True: #(batch_loss/actual_batch_factor) > 0.017:\n",
        "        #       scheduler.step()  \n",
        "        #       s  = scheduler\n",
        "        # else:  \n",
        "        #   decreasing_scheduler.step(batch_loss)\n",
        "        #   s  = decreasing_scheduler\n",
        "       \n",
        "        lrr = s.optimizer.param_groups[0]['lr']\n",
        "        for g in optimizer.param_groups:\n",
        "          g['lr'] = lrr\n",
        "\n",
        "      for g in optimizer.param_groups:\n",
        "        lr = g['lr']\n",
        "    \n",
        "    \n",
        "\n",
        "    if graph_itrator % 120 == 0:  \n",
        "      torch.save({\n",
        "              'i_batch': i_batch,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'loss': myloss,\n",
        "              'scheduler':scheduler1,                               # change here\n",
        "              'batch_loss':batch_loss\n",
        "              }, f'/content/drive/MyDrive/variant_350k_after_shape_cp1_v{n}') # final_15k_cp1_v1\n",
        "    if graph_itrator % 240 == 0: \n",
        "      torch.save({\n",
        "              'i_batch': i_batch,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'loss': myloss,\n",
        "              'scheduler':scheduler1,                             # change here\n",
        "              'batch_loss':batch_loss\n",
        "              }, f'/content/drive/MyDrive/variant_350k_after_shape_cp2_v{n}')  # final_15k_cp2_v1\n",
        "    \n",
        "    if graph_itrator % actual_batch_factor == 0:\n",
        "      graph_itrator_2 +=1 \n",
        "      writer.add_scalar('training loss',\n",
        "                            batch_loss/actual_batch_factor,\n",
        "                            (graph_itrator_2 + 26.3e3 ))\n",
        "      writer.add_scalar('lr',\n",
        "                            lr,\n",
        "                            (graph_itrator_2 + 26.3e3 ))\n",
        "      batch_loss = 0\n",
        "    graph_itrator +=1 \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHqGSQI4a6Oq",
        "outputId": "cda60cbf-df56-4b83-de16-4b3914196b39"
      },
      "source": [
        "park_dataset.sample_names.count(\"582/P_R_p2_img23_2076_aug_30\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ki5xrGL0_S2f"
      },
      "source": [
        "#prof.export_chrome_trace(\"trace.json\")\n",
        "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMMzd_5plOeX"
      },
      "source": [
        "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T84jNhIYtVWh"
      },
      "source": [
        "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHPy0wEfCAWZ"
      },
      "source": [
        "prof.export_stacks(\"/tmp/profiler_stacks.txt\", \"self_cuda_time_total\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvmdkyMop680"
      },
      "source": [
        "torch.save({\n",
        "              'i_batch': i_batch,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'loss': myloss\n",
        "              }, r'/content/drive/MyDrive/model1_checkpoint_train_20_dropout_0.01')  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "2qS-9lJplVDH",
        "outputId": "5a46c614-e9f6-495a-cc02-51d0d1696ada"
      },
      "source": [
        "while 1:\n",
        "  time.sleep(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-1d045f42a83d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoKoQdRcGq0_"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDKJI2BNu0U7"
      },
      "source": [
        "def mapper_vectorized(prediction_in):\n",
        "\n",
        "    prediction = torch.clone(prediction_in) \n",
        "    prediction[:,1:3,:,:] = ( prediction[:,1:3,:,:] * 32) + 16\n",
        "    values = torch.arange(0,512,32)\n",
        "    xv, yv = torch.meshgrid([values, values])\n",
        "    yv = yv.to(device) \n",
        "    xv = xv.to(device) \n",
        "\n",
        "    prediction[:,1,:,:] = prediction[:,1,:,:] + xv\n",
        "    prediction[:,2,:,:] = prediction[:,2,:,:] + yv\n",
        "    cos_value = prediction[:, 3, :, :] * 2\n",
        "    sin_value = prediction[:, 4, :, :] * 2\n",
        "    direction = torch.atan(sin_value / cos_value)\n",
        "    x_val_mapped = prediction[:, 1, :, :]\n",
        "    y_val_mapped = prediction[:, 2, :, :]\n",
        "    x_dir = x_val_mapped + (40 * cos_value) \n",
        "    y_dir = y_val_mapped + (40 * sin_value) \n",
        "\n",
        "    prediction[:, 3, :, :] = x_dir\n",
        "    prediction[:, 4, :, :] = y_dir\n",
        "    prediction[:, 0, :, :] = (prediction[:, 0, :, :] + 0.5) * 100\n",
        "    prediction[:, 5, :, :] = (prediction[:, 5, :, :] >= 0)\n",
        "\n",
        "    return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAeC04eT2fwZ"
      },
      "source": [
        "def predict(image):\n",
        "    out = model(image).to(device)\n",
        "    out = out.reshape((-1,6,16,16))\n",
        "    start = timeit.default_timer()\n",
        "    out = mapper_vectorized(out)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utgdbSv1Gq1A"
      },
      "source": [
        "## Visualizer 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM_Hn3tKGq1A"
      },
      "source": [
        "def visualize_after_thres(image,prediction):\n",
        "    pre = prediction\n",
        "    plt.imshow(image.permute(1, 2, 0))\n",
        "    for i in range(len(pre)):\n",
        "        plt.plot([pre[i,1]],[pre[i][2]],'o')\n",
        "        plt.plot([pre[i,3]],[pre[i][4]],'x')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGOp182yGq1A"
      },
      "source": [
        "## Remove points lower than Thresthold "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apNX4eEIGq1B"
      },
      "source": [
        "def get_predicted_points(prediction, thresh): # prediction (training,6,16,16)\n",
        "    \"\"\"Get marking points from one predicted feature map.\"\"\"\n",
        "    assert isinstance(prediction, torch.Tensor)\n",
        "    if len(prediction.shape) == 3:\n",
        "        prediction = prediction.reshape((1,6,16,16))\n",
        "    num_training_examples = prediction.shape[0]\n",
        "    predicted_points = []\n",
        "    prediction = prediction.detach().cpu()\n",
        "    \n",
        "    index = (torch.greater_equal(prediction[:,0,:,:],thresh))\n",
        "    \n",
        "    prediction = prediction.permute(0,2,3,1)\n",
        "    \n",
        "    for i in range(num_training_examples):\n",
        "        predicted_points.append(prediction[i,index[i,:,:]])\n",
        "\n",
        "    predicted_points_copy = []\n",
        "    for i in range(len(predicted_points)):\n",
        "      predicted_points_copy.append(torch.clone(predicted_points[i]))\n",
        "\n",
        "    for i in range(len(predicted_points)): # 3la el training examples\n",
        "      for j in range(len(predicted_points[i])): # 3la el points\n",
        "        if predicted_points[i][j][1] < 10 and  predicted_points[i][j][2] < 10:  # remove points with negative x , y values\n",
        "           predicted_points_copy[i] = torch.cat([predicted_points_copy[i][0:j,:], predicted_points_copy[i][j+1:,:]])\n",
        "    return (predicted_points_copy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybVT_AzAGq1C"
      },
      "source": [
        "## Remove Row from Tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTOtfoweGq1C"
      },
      "source": [
        "def remove_row_ls(tens,row_index_ls):\n",
        "    tens_copy = torch.clone(tens)\n",
        "    ls = []\n",
        "    for i in range(tens_copy.shape[0]):\n",
        "        ls.append(i)\n",
        "    ls = torch.tensor(ls)\n",
        "    #index = []\n",
        "    for i in range(len(row_index_ls)):\n",
        "        index = ls[ls!=row_index_ls[i]]\n",
        "        ls = index\n",
        "    index = torch.tensor(index)\n",
        "    return torch.index_select(tens_copy, 0, index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flFDBP-4Gq1C"
      },
      "source": [
        "## Non-max suppression for near points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBVRZnGuGq1D"
      },
      "source": [
        "def non_maximum_suppression(pred_points): # pred_points (training,num_points,6) (list)\n",
        "    \"\"\"Perform non-maxmum suppression on marking points.\"\"\"\n",
        "    pred_copy = pred_points\n",
        "    threshold_near = 40 # expermintal\n",
        "    for k , tens in enumerate(pred_copy):\n",
        "        #print(tens.shape[0])\n",
        "        index_arr = []\n",
        "        for i in range(tens.shape[0]):\n",
        "            for j in range(i + 1, tens.shape[0]):\n",
        "                i_x = tens[i][1]\n",
        "                i_y = tens[i][2]\n",
        "                j_x = tens[j][1]\n",
        "                j_y = tens[j][2]\n",
        "                abs_x = abs(j_x - i_x)\n",
        "                abs_y = abs(j_y - i_y)\n",
        "                abs_dis = math.sqrt(math.pow(abs_x,2) + math.pow(abs_y,2))\n",
        "                #print(j,abs_dis)\n",
        "                if abs_dis <= threshold_near:\n",
        "                    #print(\"close\")\n",
        "                    idx = i if tens[i][0] < tens[j][0] else j\n",
        "                    index_arr.append(idx)\n",
        "        #print(i,index_arr)            \n",
        "        #for i in range(len(index_arr)):   \n",
        "        if(len(index_arr)!=0):\n",
        "          pred_copy[k] = remove_row_ls(tens,index_arr)  \n",
        "    return pred_copy        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq-yzosaGq1D"
      },
      "source": [
        "## Calculate Distance between two points "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCuBfJCpGq1E"
      },
      "source": [
        "def calc_point_squre_dist(point_a, point_b):\n",
        "    \"\"\"Calculate distance between two marking points.\"\"\"\n",
        "    x_a = point_a[1]\n",
        "    y_a = point_a[2]\n",
        "    x_b = point_b[1]\n",
        "    y_b = point_b[2]\n",
        "\n",
        "    distx = x_a - x_b\n",
        "    disty = y_a - y_b\n",
        "    dist = math.sqrt(math.pow(distx,2) + math.pow(disty,2))\n",
        "    return dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTmeCExHAH30"
      },
      "source": [
        "i = 0 #22"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGQAWKby7TfK"
      },
      "source": [
        "for x in range(0,len(park_dataset)):\n",
        "  image_visualizer(park_dataset[x])\n",
        "  print(x)\n",
        "  #time.sleep(0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDC1xyU77NBL"
      },
      "source": [
        "model  = model.eval()\n",
        "start = timeit.default_timer()\n",
        "predict_awal = predict(park_dataset[i]['image'].reshape((1,3,512,512)).to(device))\n",
        "stop = timeit.default_timer()\n",
        "print('Time: ', stop - start)  \n",
        "predict_ba3d = get_predicted_points(predict_awal,80)\n",
        "visualize_after_thres(park_dataset[i]['image'],predict_ba3d[0])\n",
        "predict_ba3d[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcCSbRQYh_KN"
      },
      "source": [
        "# Testing Single\n",
        "image_path = r\"/content/test_2.jpg\"\n",
        "input_image = cv.imread(image_path)\n",
        "image_transform = ToTensor()\n",
        "input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n",
        "input_image = image_transform(input_image)\n",
        "# input_image = input_image.permute(1,2,0)\n",
        "# input_image = transform.resize(input_image, (512, 512))\n",
        "# input_image = torch.tensor(input_image).permute(2,0,1)\n",
        "model  = model.eval()\n",
        "start = timeit.default_timer()\n",
        "predict_awal = predict(input_image.reshape((1,3,512,512)).to(device))\n",
        "stop = timeit.default_timer()\n",
        "print('Time: ', stop - start)  \n",
        "predict_ba3d = get_predicted_points(predict_awal,50)\n",
        "predict2 = non_maximum_suppression(predict_ba3d)\n",
        "visualize_after_thres(input_image,predict2[0])\n",
        "print(predict2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nMAJ3P_a0vX"
      },
      "source": [
        "# Testing data\n",
        "root_path = r'/content/drive/MyDrive/2esmtany8arbev'\n",
        "names = []\n",
        "for file in os.listdir(root_path):\n",
        "            if file.endswith(\".jpg\"):\n",
        "                names.append(os.path.splitext(file)[0])\n",
        "for i in range(0,len(names)):\n",
        "  image_path = f\"{root_path}/{names[i]}.jpg\"\n",
        "  input_image = cv.imread(image_path)\n",
        "  input_image = cv2.medianBlur(input_image,5)\n",
        "  smoothed = cv2.GaussianBlur(input_image, (9, 9), 10)\n",
        "  input_image = cv2.addWeighted(input_image, 2, smoothed, -1, 5)\n",
        "  image_transform = ToTensor()\n",
        "  input_image = cv2.cvtColor(input_image, cv2.COLOR_BGR2RGB)\n",
        "  input_image = image_transform(input_image)\n",
        "  #input_image = input_image.permute(1,2,0)\n",
        "  #input_image = transform.resize(input_image, (512, 512))\n",
        "  #input_image = torch.tensor(input_image).permute(2,0,1)\n",
        "  model  = model.eval()\n",
        "  start = timeit.default_timer()\n",
        "  predict_awal = predict(input_image.reshape((1,3,512,512)).to(device))\n",
        "  stop = timeit.default_timer()\n",
        "  print('Time: ', stop - start)  \n",
        "  predict_ba3d = get_predicted_points(predict_awal,70)\n",
        "  predict2 = non_maximum_suppression(predict_ba3d)\n",
        "  visualize_after_thres(input_image,predict2[0])\n",
        "  print(i,predict2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD7-Hwtmkwa5",
        "outputId": "caf980ec-ce85-480c-9f57-814ce956c3d4"
      },
      "source": [
        "park_dataset.change_mode(\"train\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training... \n",
            "Training Dataset length is: 309643\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edJ5TjJt0Mdk"
      },
      "source": [
        "for j in range(2000,len(park_dataset)):\n",
        "  model  = model.eval()\n",
        "  start = timeit.default_timer()\n",
        "  predict_awal = predict(park_dataset[j]['image'].reshape((1,3,512,512)).to(device))\n",
        "  stop = timeit.default_timer()\n",
        "  predict_ba3d = get_predicted_points(predict_awal,80)\n",
        "  predict2 = non_maximum_suppression(predict_ba3d)\n",
        "  visualize_after_thres(park_dataset[j]['image'],predict2[0])\n",
        "  print(j,predict2[0])\n",
        "  #time.sleep(0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeLbdoGbk8r5",
        "outputId": "878f7e16-4b1e-4c3f-881a-fd120dc48099"
      },
      "source": [
        "park_dataset.change_mode(\"test\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing... \n",
            "Testing Dataset length is: 54644\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-Gwt3oTkshI"
      },
      "source": [
        "for j in range(3000,len(park_dataset)):\n",
        "  model  = model.eval()\n",
        "  start = timeit.default_timer()\n",
        "  predict_awal = predict(park_dataset[j]['image'].reshape((1,3,512,512)).to(device))\n",
        "  stop = timeit.default_timer()\n",
        "  predict_ba3d = get_predicted_points(predict_awal,80)\n",
        "  predict2 = non_maximum_suppression(predict_ba3d)\n",
        "  visualize_after_thres(park_dataset[j]['image'],predict2[0])\n",
        "  print(j,predict2[0])\n",
        "  #time.sleep(0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJE-mUxa7C3i"
      },
      "source": [
        "predict_ba3d[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxDrcJ0eklxC"
      },
      "source": [
        "predict2 = non_maximum_suppression(predict_ba3d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX99IvaWkySM"
      },
      "source": [
        "visualize_after_thres(park_dataset[i]['image'],predict2[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxNJ6n4S5qF4"
      },
      "source": [
        "predict_ba3d[0][:,1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QLKye_45wPc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhHCIsauGq1E"
      },
      "source": [
        "## Is there a third point that Passes through a line between these two points? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w9HJAR7Gq1E"
      },
      "source": [
        "SLOT_SUPPRESSION_DOT_PRODUCT_THRESH = 0.8\n",
        "def pass_through_third_point(marking_points, i, j):\n",
        "    \"\"\"See whether the line between two points pass through a third point.\"\"\"\n",
        "    x_1 = marking_points[i][1]\n",
        "    y_1 = marking_points[i][2]\n",
        "    x_2 = marking_points[j][1]\n",
        "    y_2 = marking_points[j][2]\n",
        "    for point_idx, tens in enumerate(marking_points):\n",
        "        if point_idx == i or point_idx == j:\n",
        "            continue\n",
        "        x_0 = tens[1]\n",
        "        y_0 = tens[2]\n",
        "        vec1 = np.array([x_0 - x_1, y_0 - y_1])\n",
        "        vec2 = np.array([x_2 - x_0, y_2 - y_0])\n",
        "        vec1 = vec1 / np.linalg.norm(vec1)\n",
        "        vec2 = vec2 / np.linalg.norm(vec2)\n",
        "        if np.dot(vec1, vec2) > config.SLOT_SUPPRESSION_DOT_PRODUCT_THRESH:\n",
        "            return True\n",
        "    return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtKcfT_Fpr92"
      },
      "source": [
        "epsilon = 0.001  # check on number\n",
        "\n",
        "# check if c between a,b\n",
        "def isBetween(a, b, c):\n",
        "    crossproduct = (c[2] - a[2]) * (b[1] - a[1]) - (c[1] - a[1]) * (b[2] - a[2])\n",
        "    print (\"crossproduct\",crossproduct)\n",
        "    # compare versus epsilon for floating point values, or != 0 if using integers \n",
        "    # sin 0 =0 , check theta >0 (not parallel)\n",
        "    if abs(crossproduct) > epsilon:\n",
        "        return False\n",
        "\n",
        "    # cos 180 = -1, check that they are in the same direction\n",
        "    dotproduct = (c[1] - a[1]) * (b[1] - a[1]) + (c[2] - a[2])*(b[2] - a[2])\n",
        "    print (\"dotproduct\",dotproduct)\n",
        "    if dotproduct < 0:\n",
        "        return False\n",
        "\n",
        "    squaredlengthba = (b[1] - a[1])*(b[1] - a[1]) + (b[2] - a[2])*(b[2] - a[2])\n",
        "    print (\"squaredlengthba\",squaredlengthba)\n",
        "\n",
        "    if dotproduct > squaredlengthba:\n",
        "        return False\n",
        "\n",
        "    return True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7JRBVWEwswj"
      },
      "source": [
        "a = [1,0,2]\n",
        "b = [1,0,6]\n",
        "c = [1,0,4]\n",
        "\n",
        "test = isBetween (a,b,c)\n",
        "print(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-0bQ55gtALI"
      },
      "source": [
        "decreased_points = non_maximum_suppression(outputs)\n",
        "# pairing point \n",
        "for k , tens in enumerate(decreased_points):\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvmI5lrcGq1F"
      },
      "source": [
        "## Calculate Direction(angle)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FirSpR4HGq1F"
      },
      "source": [
        "def calc_angle(p1,p2):\n",
        "  y_diff = p2[1]- p1[1]\n",
        "  x_diff = p2[0] - p1[0]\n",
        "  if x_diff == 0 and y_diff ==0 :\n",
        "    return 0\n",
        "  theta = math.atan(abs( y_diff/x_diff)) * (180/math.pi)\n",
        "  \n",
        "  \n",
        "  if x_diff >=0 and y_diff >=0:\n",
        "    return theta\n",
        "  elif x_diff < 0 and y_diff >=0:\n",
        "    return 180- theta\n",
        "  elif x_diff < 0 and y_diff < 0:\n",
        "    return  180 - theta\n",
        "  else:\n",
        "    return theta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5EkPBFGGq1G"
      },
      "source": [
        "## Calculate difference in direction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMh42tD_Gq1G"
      },
      "source": [
        "def direction_diff(direction_a, direction_b):\n",
        "    \"\"\"Calculate the angle between two direction.\"\"\"\n",
        "    diff = abs(direction_a - direction_b)\n",
        "    return diff if diff < math.pi else 2*math.pi - diff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENV0YrjhGq1G"
      },
      "source": [
        "## Determine Point Shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wakZ2SmWGq1G"
      },
      "source": [
        "none = 0\n",
        "l_down = 1\n",
        "t_down = 2\n",
        "t_middle = 3\n",
        "t_up = 4\n",
        "l_up = 5\n",
        "BRIDGE_ANGLE_DIFF = 0.09757113548987695 + 0.1384059287593468\n",
        "SEPARATOR_ANGLE_DIFF = 0.284967562063968 + 0.1384059287593468\n",
        "\n",
        "def detemine_point_shape(point, vector):\n",
        "    \"\"\"Determine which category the point is in.\"\"\"\n",
        "    vec_direct = math.atan2(vector[1], vector[0])\n",
        "    vec_direct_up = math.atan2(-vector[0], vector[1])\n",
        "    vec_direct_down = math.atan2(vector[0], -vector[1])\n",
        "    direction =  calc_angle ([point[1],point[2]],[point[3],point[4]])\n",
        "    direction = (direction * math.pi)/180\n",
        "    if point[5] < 0.5:\n",
        "        if direction_diff(vec_direct, direction) < BRIDGE_ANGLE_DIFF:\n",
        "            return t_middle\n",
        "        if direction_diff(vec_direct_up, direction) < SEPARATOR_ANGLE_DIFF:\n",
        "            return t_up\n",
        "        if direction_diff(vec_direct_down, direction) < SEPARATOR_ANGLE_DIFF:\n",
        "            return t_down\n",
        "    else:\n",
        "        if direction_diff(vec_direct, direction) < BRIDGE_ANGLE_DIFF:\n",
        "            return l_down\n",
        "        if direction_diff(vec_direct_up, direction) < SEPARATOR_ANGLE_DIFF:\n",
        "            return l_up\n",
        "    return none"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hxifguLGq1G"
      },
      "source": [
        "## Pair Marking Points to form slots (can these two make a slot)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WL4obe09Gq1H"
      },
      "source": [
        "def pair_marking_points(point_a, point_b):\n",
        "    \"\"\"See whether two marking points form a slot.\"\"\"\n",
        "    vector_ab = np.array([point_b[1] - point_a[1], point_b[2] - point_a[2]])\n",
        "    vector_ab = vector_ab / np.linalg.norm(vector_ab)\n",
        "    point_shape_a = detemine_point_shape(point_a, vector_ab)\n",
        "    point_shape_b = detemine_point_shape(point_b, -vector_ab)\n",
        "    if point_shape_a == 0 or point_shape_b == 0:\n",
        "        return 0\n",
        "    if point_shape_a == 3 and point_shape_b == 3:\n",
        "        return 0\n",
        "    if point_shape_a > 3 and point_shape_b > 3:\n",
        "        return 0\n",
        "    if point_shape_a < 3 and point_shape_b < 3:\n",
        "        return 0\n",
        "    if point_shape_a != 3:\n",
        "        if point_shape_a > 3:\n",
        "            return 1\n",
        "        if point_shape_a < 3:\n",
        "            return -1\n",
        "    if point_shape_a == 3:\n",
        "        if point_shape_b < 3:\n",
        "            return 1\n",
        "        if point_shape_b > 3:\n",
        "            return -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDva8k_0Gq1H"
      },
      "source": [
        "## Knowing Marking Points Get Slots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYfyGvo7Gq1H"
      },
      "source": [
        "perpend_min = 0.044771278151623496 *600 #26.862\n",
        "perpend_max = 0.1099427457599304 *600   #65.96\n",
        "parallel_min = 0.15057789144568634 *600 # 90.34\n",
        "parallel_max = 350 # 266.69\n",
        "\n",
        "\n",
        "def inference_slots(marking_points):\n",
        "    \"\"\"Inference slots based on marking points.\"\"\"\n",
        "    num_detected = marking_points.shape[0]\n",
        "    perpen_parallel = 0  # perpendicular = 1 , parallel = 2\n",
        "    \n",
        "    slots ={}\n",
        "    for i in range(num_detected - 1):\n",
        "        for j in range(i + 1, num_detected):\n",
        "            print (i,j)\n",
        "            point_i = marking_points[i]\n",
        "            point_j = marking_points[j]\n",
        "            # Step 1: length filtration.\n",
        "            distance = calc_point_squre_dist(point_i, point_j)\n",
        "            print (\"distance\",distance)\n",
        "            if not (perpend_min <= distance <= perpend_max\n",
        "                    or parallel_min <= distance <= parallel_max):\n",
        "\n",
        "                continue\n",
        "                \n",
        "            # Step 2: pass through filtration.\n",
        "            if pass_through_third_point(marking_points, i, j):\n",
        "                continue\n",
        "            result = pair_marking_points(point_i, point_j)\n",
        "            if  (perpend_min <= distance <= perpend_max):\n",
        "                perpen_parallel = 1 # perpendiculer\n",
        "            elif  (parallel_min <= distance <= parallel_max):\n",
        "                perpen_parallel = 2 # parallel\n",
        "\n",
        "            print(\"result\",result)\n",
        "            test1 ={'x1': marking_points[i][1],'y1': marking_points[i][2],'x2': marking_points[j][1],'y2': marking_points[j][2], 'type': perpen_parallel}\n",
        "            test2 ={'x1': marking_points[j][1],'y1': marking_points[j][2],'x2': marking_points[i][1],'y2': marking_points[i][2], 'type': perpen_parallel}\n",
        "            if result == 1:\n",
        "               slots.update(test1)\n",
        "            elif result == -1:\n",
        "                slots.update(test2)\n",
        "    return slots"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EyPrDMT036T"
      },
      "source": [
        "inference_slots(predict_ba3d[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4CBQ0PA1VnS"
      },
      "source": [
        "predict_ba3d[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOYVhrk-LfXn"
      },
      "source": [
        "#monica\n",
        "image = park_dataset [0]['image']\n",
        "image.reshape((1,3,512,512))\n",
        "predicted = predict(image)\n",
        "y = get_predicted_points (predicted,threshold)\n",
        "test = visualize_after_thres(image,y[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKk40LOCNd2a"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEEnmTvRGq1I"
      },
      "source": [
        "#outputs = process(outputs) # take marking points outputs and remove useless ones , non max suppression and do slot pairing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAO3CeRlGq1J"
      },
      "source": [
        "# get slots\n",
        "# outputs slots\n",
        "# train\n",
        "# evalute"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUHgcQh73-ts"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}